\documentclass[landscape,a0paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{url}

\geometry{margin=2cm}
\pagestyle{empty}

\definecolor{lightblue}{RGB}{200,220,240}
\definecolor{darkblue}{RGB}{50,100,150}

\newcommand{\posterbox}[2]{
    \begin{minipage}[t]{0.32\textwidth}
        \vspace{0.5em}
        \fcolorbox{darkblue}{lightblue}{
            \begin{minipage}{0.95\textwidth}
                \vspace{0.3em}
                \textbf{\large #1}\\
                \rule{0.9\textwidth}{0.5pt}\\
                \vspace{0.3em}
                #2
            \end{minipage}
        }
    \end{minipage}
}

\begin{document}

% Title Section
\begin{center}
\vspace{0.2cm}
{\Huge \textbf{Stability and Faithfulness Analysis of SHAP Explanations}}\\[0.15cm]
{\Large \textit{for Tabular Machine Learning Models}}\\[0.3cm]
{\large Keisuke Nishioka (Matrikelnummer: 10081049)}\\[0.1cm]
{\small Interpretierbares Maschinelles Lernen | Prof. Dr. rer. nat. Marius Lindauer}\\[0.2cm]
\rule{0.95\textwidth}{1.5pt}
\end{center}

\vspace{0.2cm}

% First Row
\begin{center}
\posterbox{Introduction}{
\vspace{0.3em}

\textbf{Research Question:}\\
How stable and faithful are SHAP explanations across different random seeds, dataset sizes, and model classes?

\vspace{0.5em}

\textbf{Motivation:}
\begin{itemize}[leftmargin=*]
    \item SHAP is widely used in production systems
    \item Explanation stability is crucial for reliable interpretation
    \item Understanding stability conditions helps practitioners
\end{itemize}

\vspace{0.5em}

\textbf{Key Concepts:}
\begin{itemize}[leftmargin=*]
    \item \textbf{SHAP}: SHapley Additive exPlanations
    \item \textbf{TreeSHAP}: For tree-based models (exact)
    \item \textbf{KernelSHAP}: For any model (approximation)
    \item \textbf{Stability}: Consistency under perturbations
\end{itemize}
}
\hfill
\posterbox{Key Results}{
\vspace{0.3em}

\textbf{Stability Metrics Comparison:}

\begin{center}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Ranking} & \textbf{SHAP} & \textbf{Top-5} \\
 & \textbf{Corr.} & \textbf{Var.} & \textbf{Consist.} \\
\midrule
Random Forest & \textbf{0.909} & \textbf{0.00016} & 0.353 \\
XGBoost & 0.562 & 0.00033 & \textbf{0.427} \\
Logistic Reg. & 0.616 & 0.00030 & 0.127 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}

\textbf{Key Findings:}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Random Forest} shows highest ranking correlation (0.909)
    \item All models show low SHAP variance ($<$ 0.0004)
    \item \textbf{XGBoost} shows best top-5 consistency (0.427)
    \item Ensemble methods provide more stable explanations
\end{enumerate}
}
\hfill
\posterbox{Methodology}{
\vspace{0.3em}

\textbf{Dataset:}\\
Wine Quality (UCI Repository)\\
Binary classification task

\vspace{0.5em}

\textbf{Models:}
\begin{itemize}[leftmargin=*]
    \item XGBoost (TreeSHAP)
    \item Random Forest (TreeSHAP)
    \item Logistic Regression (KernelSHAP)
\end{itemize}

\vspace{0.5em}

\textbf{SHAP Value:}
\begin{itemize}[leftmargin=*]
    \item $\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f_{S \cup \{i\}} - f_S]$
\end{itemize}

\vspace{0.5em}

\textbf{Evaluation Metrics:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Ranking Correlation}: Spearman $\rho$ (0-1)
    \item \textbf{SHAP Variance}: $\text{Var}(\phi_i)$
    \item \textbf{Top-k Consistency}: $\%$ consistent features
\end{itemize}

\vspace{0.5em}

\textbf{Experimental Setup:}
\begin{itemize}[leftmargin=*]
    \item Random seeds: 42, 123, 456
    \item 100 test instances analyzed
    \item Top-k: 3, 5, 10 features
\end{itemize}
}
\end{center}

\vspace{0.2cm}

% Second Row
\begin{center}
\posterbox{Visualizations}{
\vspace{0.3em}

\begin{center}
\includegraphics[width=0.95\textwidth]{results/figures/model_comparison.png}
\vspace{0.3em}

\textit{Model Stability Comparison}
\end{center}

\vspace{0.5em}

\begin{center}
\includegraphics[width=0.30\textwidth]{results/figures/random_forest_shap_summary.png}
\includegraphics[width=0.30\textwidth]{results/figures/xgboost_shap_summary.png}
\includegraphics[width=0.30\textwidth]{results/figures/logistic_regression_shap_summary.png}
\vspace{0.3em}

\textit{SHAP Summary: RF, XGBoost, Logistic Regression}
\end{center}

}
\hfill
\posterbox{Detailed Analysis}{
\vspace{0.3em}

\begin{center}
\includegraphics[width=0.42\textwidth]{results/figures/random_forest_consistency.png}
\includegraphics[width=0.42\textwidth]{results/figures/xgboost_consistency.png}
\vspace{0.2em}

\textit{Consistency: RF (left), XGBoost (right)}
\end{center}

\vspace{0.2em}

\textbf{Model Insights:}
\begin{itemize}[leftmargin=*]
    \item \textbf{RF}: Highest stability (0.909), ensemble reduces variance
    \item \textbf{XGBoost}: Moderate (0.562), best top-5 consistency (0.427)
    \item \textbf{LR}: Lower consistency, KernelSHAP variability
\end{itemize}
}
\hfill
\posterbox{Subsampling Analysis}{
\vspace{0.3em}

\begin{center}
\includegraphics[width=0.95\textwidth]{results/figures/subsampling_analysis.png}
\vspace{0.3em}

\textit{Stability vs. Sample Size}
\end{center}

\vspace{0.3em}

\textbf{Ranking Correlation by Sample Size:}
\begin{center}
\tiny
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
RF & 0.912 & 0.915 & 0.913 \\
XGB & 0.551 & 0.549 & 0.528 \\
LR & 0.638 & 0.621 & 0.624 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.2em}

\textbf{Key Findings:}
\begin{itemize}[leftmargin=*]
    \item RF: High stability (0.91+) across all sizes
    \item Sample size: \textbf{Limited impact} on stability
    \item Architecture: Primary factor
\end{itemize}
}
\hfill
\posterbox{Conclusions \& Recommendations}{
\vspace{0.3em}

\textbf{Main Conclusions:}
\begin{enumerate}[leftmargin=*]
    \item Random Forest provides the most stable SHAP explanations
    \item Sample size has limited impact on explanation stability
    \item Model architecture significantly impacts stability
    \item Ensemble methods offer superior stability
\end{enumerate}

\vspace{0.2em}

\textbf{Recommendations:}
\begin{itemize}[leftmargin=*]
    \item Use \textbf{Random Forest} with TreeSHAP for critical applications
    \item Focus on \textbf{top-5 or top-10} features for stable insights
    \item Sample size (50\%-100\%) has \textbf{minimal impact} on stability
    \item Run \textbf{multiple analyses} with different seeds
\end{itemize}
}

\vspace{0.5cm}

% Footer
\begin{center}
\rule{0.95\textwidth}{0.5pt}\\[0.3cm]
{\small \textit{Interpretierbares Maschinelles Lernen (WiSe 2025/26) | Keisuke Nishioka (Matrikelnummer: 10081049)}}\\[0.1cm]
{\tiny \textit{Code: \url{https://github.com/keisuke58/shap-stability-analysis}}}
\end{center}

\vspace{0.3cm}

\end{document}
